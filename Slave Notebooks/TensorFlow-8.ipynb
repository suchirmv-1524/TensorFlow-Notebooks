{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a research paper implementation of medical data which is abstract to comprehend\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'pubmed-rct' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_text_with_line_numbers(filename):\n",
    "    input_lines = get_lines(filename)\n",
    "    abstract_lines = \"\"\n",
    "    abstract_samples = []\n",
    "    for line in input_lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_id = line\n",
    "            abstract_lines=\"\"\n",
    "        elif line.isspace():\n",
    "            abstract_line_split = abstract_lines.splitlines()\n",
    "\n",
    "            for abstract_line_number, abstract_line in enumerate(abstract_line_split):\n",
    "                line_data = {}\n",
    "                target_text_split = abstract_line.split(\"\\t\")\n",
    "                line_data[\"target\"] = target_text_split[0]\n",
    "                line_data[\"text\"] = target_text_split[1].lower()\n",
    "                line_data[\"line_number\"] = abstract_line_number\n",
    "                line_data[\"total_lines\"] = len(abstract_line_split)-1\n",
    "                abstract_samples.append(line_data)\n",
    "        else:\n",
    "            abstract_lines+=line\n",
    "    return abstract_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/dev.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/train.txt',\n",
       " 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/test.txt']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "data_dir = 'pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/'\n",
    "filenames = [data_dir + filename for filename in os.listdir(data_dir)]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24290286\\n',\n",
       " 'BACKGROUND\\tIgE sensitization to Aspergillus fumigatus and a positive sputum fungal culture result are common in patients with refractory asthma .\\n',\n",
       " 'BACKGROUND\\tIt is not clear whether these patients would benefit from antifungal treatment .\\n',\n",
       " 'OBJECTIVE\\tWe sought to determine whether a @-month course of voriconazole improved asthma-related outcomes in patients with asthma who are IgE sensitized to A fumigatus .\\n',\n",
       " 'METHODS\\tAsthmatic patients who were IgE sensitized to A fumigatus with a history of at least @ severe exacerbations in the previous @ months were treated for @ months with @ mg of voriconazole twice daily , followed by observation for @ months , in a double-blind , placebo-controlled , randomized design .\\n',\n",
       " 'METHODS\\tPrimary outcomes were improvement in quality of life at the end of the treatment period and a reduction in the number of severe exacerbations over the @ months of the study .\\n',\n",
       " 'RESULTS\\tSixty-five patients were randomized .\\n',\n",
       " 'RESULTS\\tFifty-nine patients started treatment ( @ receiving voriconazole and @ receiving placebo ) and were included in an intention-to-treat analysis .\\n',\n",
       " 'RESULTS\\tFifty-six patients took the full @ months of medication .\\n',\n",
       " 'RESULTS\\tBetween the voriconazole and placebo groups , there were no significant differences in the number of severe exacerbations ( @ vs @ per patient per year , respectively ; mean difference , @ ; @ % CI , @-@ @ ) , quality of life ( change in Asthma Quality of Life Questionnaire score , @ vs @ ; mean difference between groups , @ ; @ % CI , -@ to -@ ) , or any of our secondary outcome measures .\\n',\n",
       " 'CONCLUSIONS\\tWe were unable to show a beneficial effect of @ months of treatment with voriconazole in patients with moderate-to-severe asthma who were IgE sensitized to A fumigatus on either the rate of severe exacerbations , quality of life , or other markers of asthma control .\\n',\n",
       " '\\n',\n",
       " '###24464531\\n',\n",
       " 'BACKGROUND\\tOpioid antagonists ( e.g. , naltrexone ) and positive modulators of - aminobutyric-acidA ( GABAA ) receptors ( e.g. , alprazolam ) modestly attenuate the abuse-related effects of stimulants like amphetamine .\\n',\n",
       " 'BACKGROUND\\tThe use of higher doses to achieve greater efficacy is precluded by side effects .\\n',\n",
       " 'BACKGROUND\\tCombining naltrexone and alprazolam might safely maximize efficacy while avoiding the untoward effects of the constituent compounds .\\n',\n",
       " 'OBJECTIVE\\tThe present pilot study tested the hypothesis that acute pretreatment with the combination of naltrexone and alprazolam would not produce clinically problematic physiological effects or negative subjective effects and would reduce the positive subjective effects of d-amphetamine to a greater extent than the constituent drugs alone .\\n',\n",
       " 'METHODS\\tEight nontreatment-seeking , stimulant-using individuals completed an outpatient experiment in which oral d-amphetamine ( @ , @ , and @ mg ) was administered following acute pretreatment with naltrexone ( @ and @ mg ) and alprazolam ( @ and @ mg ) .\\n',\n",
       " 'METHODS\\tSubjective effects , psychomotor task performance , and physiological measures were collected .\\n',\n",
       " 'RESULTS\\tOral d-amphetamine produced prototypical physiological and stimulant-like positive subjective effects ( e.g. , VAS ratings of Active/Alert/Energetic , Good Effect , and High ) .\\n']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lines(filenames[0])[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = preprocess_text_with_line_numbers(filenames[0])\n",
    "val_samples = preprocess_text_with_line_numbers(filenames[1])\n",
    "test_samples = preprocess_text_with_line_numbers(filenames[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'target': 'BACKGROUND',\n",
       "  'text': 'ige sensitization to aspergillus fumigatus and a positive sputum fungal culture result are common in patients with refractory asthma .',\n",
       "  'line_number': 0,\n",
       "  'total_lines': 9},\n",
       " {'target': 'BACKGROUND',\n",
       "  'text': 'it is not clear whether these patients would benefit from antifungal treatment .',\n",
       "  'line_number': 1,\n",
       "  'total_lines': 9},\n",
       " {'target': 'OBJECTIVE',\n",
       "  'text': 'we sought to determine whether a @-month course of voriconazole improved asthma-related outcomes in patients with asthma who are ige sensitized to a fumigatus .',\n",
       "  'line_number': 2,\n",
       "  'total_lines': 9}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_number</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>ige sensitization to aspergillus fumigatus and...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>it is not clear whether these patients would b...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OBJECTIVE</td>\n",
       "      <td>we sought to determine whether a @-month cours...</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>asthmatic patients who were ige sensitized to ...</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>primary outcomes were improvement in quality o...</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               text  line_number  \\\n",
       "0  BACKGROUND  ige sensitization to aspergillus fumigatus and...            0   \n",
       "1  BACKGROUND  it is not clear whether these patients would b...            1   \n",
       "2   OBJECTIVE  we sought to determine whether a @-month cours...            2   \n",
       "3     METHODS  asthmatic patients who were ige sensitized to ...            3   \n",
       "4     METHODS  primary outcomes were improvement in quality o...            4   \n",
       "\n",
       "   total_lines  \n",
       "0            9  \n",
       "1            9  \n",
       "2            9  \n",
       "3            9  \n",
       "4            9  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ige sensitization to aspergillus fumigatus and a positive sputum fungal culture result are common in patients with refractory asthma .',\n",
       " 'it is not clear whether these patients would benefit from antifungal treatment .']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences = train_df.text.to_list()\n",
    "val_sentences = val_df.text.to_list()\n",
    "test_sentences = test_df.text.to_list()\n",
    "\n",
    "train_sentences[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(sparse_output = False) \n",
    "\n",
    "train_labels_one_hot = one_hot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1, 1))\n",
    "val_labels_one_hot = one_hot_encoder.fit_transform(val_df['target'].to_numpy().reshape(-1, 1))\n",
    "test_labels_one_hot = one_hot_encoder.fit_transform(test_df['target'].to_numpy().reshape(-1, 1))\n",
    "\n",
    "train_labels_one_hot #binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 3, ..., 4, 1, 1])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "train_labels_encoded = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
    "val_labels_encoded = label_encoder.fit_transform(val_df['target'].to_numpy())\n",
    "test_labels_encoded = label_encoder.fit_transform(test_df['target'].to_numpy())\n",
    "\n",
    "train_labels_encoded #decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['BACKGROUND', 'CONCLUSIONS', 'METHODS', 'OBJECTIVE', 'RESULTS'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(label_encoder.classes_)\n",
    "class_names = label_encoder.classes_\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "def calculate_results(y_true, y_pred):\n",
    "  \"\"\"\n",
    "  Calculates model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "\n",
    "  Args:\n",
    "      y_true: true labels in the form of a 1D array\n",
    "      y_pred: predicted labels in the form of a 1D array\n",
    "\n",
    "  Returns a dictionary of accuracy, precision, recall, f1-score.\n",
    "  \"\"\"\n",
    "  # Calculate model accuracy\n",
    "  model_accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "  # Calculate model precision, recall and f1 score using \"weighted average\n",
    "  model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "  model_results = {\"accuracy\": model_accuracy,\n",
    "                  \"precision\": model_precision,\n",
    "                  \"recall\": model_recall,\n",
    "                  \"f1\": model_f1}\n",
    "  return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 67.64107976005333,\n",
       " 'precision': 0.6805658878020778,\n",
       " 'recall': 0.6764107976005332,\n",
       " 'f1': 0.6349298999118057}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#model_0 uses Naive Bayes Model\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "model_0.fit(train_sentences, train_labels_encoded)\n",
    "model_0.results = calculate_results(val_labels_encoded, model_0.predict(val_sentences))\n",
    "model_0.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_length = [len(sentence.split()) for sentence in train_sentences]\n",
    "tf.math.reduce_mean(sent_length).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "int(np.percentile(sent_length, 95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 55), dtype=int64, numpy=\n",
       "array([[4139,   63, 1093,   64,    1,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens = 68000,\n",
    "    output_sequence_length = 55\n",
    ") #55 is the 95 percentile of sent_length\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "sample_sentence = 'hi there, how are you'\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 55, 120), dtype=float32, numpy=\n",
       "array([[[-0.02091458, -0.02848719, -0.03392887, ..., -0.04245558,\n",
       "          0.02222841,  0.00402117],\n",
       "        [ 0.04975675, -0.01769758,  0.00190584, ..., -0.00498702,\n",
       "         -0.03638126, -0.02260991],\n",
       "        [ 0.01655388, -0.04954735,  0.04931574, ..., -0.01548467,\n",
       "          0.04112792,  0.01318311],\n",
       "        ...,\n",
       "        [ 0.01021777,  0.04414475,  0.04980525, ..., -0.04040786,\n",
       "          0.01880119,  0.01564344],\n",
       "        [ 0.01021777,  0.04414475,  0.04980525, ..., -0.04040786,\n",
       "          0.01880119,  0.01564344],\n",
       "        [ 0.01021777,  0.04414475,  0.04980525, ..., -0.04040786,\n",
       "          0.01880119,  0.01564344]]], dtype=float32)>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = tf.keras.layers.Embedding(\n",
    "    input_dim = 68000,\n",
    "    output_dim = 120,\n",
    "    input_length = 55,\n",
    "    mask_zero = True\n",
    ")\n",
    "\n",
    "embedding(text_vectorizer([sample_sentence]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=(TensorSpec(shape=(), dtype=tf.string, name=None), TensorSpec(shape=(5,), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None, 5), dtype=tf.float64, name=None))>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(64).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "473/473 [==============================] - 12s 24ms/step - loss: 0.4308 - accuracy: 0.8758 - val_loss: 0.8299 - val_accuracy: 0.7378\n",
      "Epoch 2/5\n",
      "473/473 [==============================] - 11s 23ms/step - loss: 0.1724 - accuracy: 0.9495 - val_loss: 1.0435 - val_accuracy: 0.7275\n",
      "Epoch 3/5\n",
      "473/473 [==============================] - 11s 23ms/step - loss: 0.1220 - accuracy: 0.9646 - val_loss: 1.2247 - val_accuracy: 0.7228\n",
      "Epoch 4/5\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0938 - accuracy: 0.9738 - val_loss: 1.4020 - val_accuracy: 0.7152\n",
      "Epoch 5/5\n",
      "473/473 [==============================] - 11s 24ms/step - loss: 0.0775 - accuracy: 0.9786 - val_loss: 1.5524 - val_accuracy: 0.7107\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (1, ), dtype = 'string')\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(64, kernel_size = 5)(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "outputs = tf.keras.layers.Dense(num_classes, activation = 'softmax')(x)\n",
    "\n",
    "model_1 = tf.keras.models.Model(inputs, outputs) #model_1 uses CNN\n",
    "\n",
    "model_1.compile(\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "    optimizer = tf.keras.optimizers.Adam(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history_1 = model_1.fit(\n",
    "    train_dataset,\n",
    "    epochs = 5,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = int(0.2 * len(val_dataset))\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2814/2814 [==============================] - 6s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 73.0254387913797,\n",
       " 'precision': 0.7386353504832475,\n",
       " 'recall': 0.7302543879137969,\n",
       " 'f1': 0.7332701444386144}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pred_probs = model_1.predict(val_dataset)\n",
    "model_1_preds = tf.argmax(model_pred_probs, axis = 1).numpy()\n",
    "model_1_results = calculate_results(val_labels_encoded, model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_10 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_3 (Text  (None, 55)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 55, 120)           8160000   \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 51, 64)            38464     \n",
      "                                                                 \n",
      " global_average_pooling1d_9  (None, 64)                0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8198789 (31.28 MB)\n",
      "Trainable params: 8198789 (31.28 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_2 uses Universal Sentence Encoder\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to observe the protective effect of transcutaneous electrical acupoint stimulation ( teas ) on cerebral tissue in elderly hip replacement operation patients during general anesthesia under controlled hypotension . \n",
      " 213\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 55), dtype=int64, numpy=\n",
       "array([[   6, 2107,    2, 2153,   74,    4, 4547, 2751, 5222,  703, 5755,\n",
       "          19, 1081,  631,    5,  781, 1183, 1142,  860,   11,   56,  309,\n",
       "         419,  341,  101, 1496,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(random_sentence,'\\n' ,len(random_sentence))\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 512), dtype=float32, numpy=\n",
       "array([[ 0.02192686,  0.0717385 ,  0.06441112, -0.0727673 , -0.02691519,\n",
       "        -0.00605917,  0.03653429, -0.0550508 , -0.02536041,  0.02767338,\n",
       "         0.0799851 ,  0.05593303,  0.02379033,  0.06863826,  0.02127163,\n",
       "        -0.06800511, -0.08003996, -0.02161273, -0.06571784, -0.04634899,\n",
       "        -0.03561794, -0.00475984, -0.02629415, -0.03132291,  0.02071399,\n",
       "         0.01995087, -0.06193408,  0.0269776 , -0.0172271 ,  0.04608248,\n",
       "         0.02273444,  0.08011284,  0.07033496, -0.05087849,  0.03109113,\n",
       "         0.02127643,  0.02391165, -0.03701675, -0.03789214, -0.06972475,\n",
       "         0.05086392,  0.00221968,  0.05823132, -0.0681831 ,  0.0267735 ,\n",
       "         0.07565604, -0.01899219, -0.03650256,  0.03285763, -0.02870599,\n",
       "        -0.07292692,  0.01659679,  0.02373334,  0.04792888, -0.06238138,\n",
       "         0.0252289 , -0.03569742,  0.02599585,  0.05232928, -0.00568793,\n",
       "        -0.03624795,  0.07307966,  0.00145131,  0.04784302,  0.0280991 ,\n",
       "        -0.02154975,  0.01428313,  0.0247947 ,  0.01134464, -0.00939778,\n",
       "         0.03135978,  0.00059134, -0.03018762, -0.07101713, -0.05187635,\n",
       "        -0.04708898,  0.04874393,  0.02408775,  0.01606108, -0.05325078,\n",
       "        -0.03591254,  0.01115507,  0.00810166,  0.04535575, -0.07392087,\n",
       "         0.03451153,  0.03028652, -0.02081064,  0.06746332,  0.03593592,\n",
       "        -0.04320952, -0.00866502,  0.00668438, -0.01008401, -0.04987652,\n",
       "        -0.05580559, -0.05238696,  0.00769237,  0.0229534 , -0.00260064,\n",
       "         0.01471582, -0.02395539,  0.06268648,  0.0504707 , -0.03483782,\n",
       "        -0.07944954,  0.05250813, -0.03338708, -0.04724252, -0.05013954,\n",
       "         0.04938908,  0.06013466,  0.05418475, -0.05718606, -0.03335453,\n",
       "         0.04142788,  0.00630862,  0.00398079,  0.03862952,  0.04667091,\n",
       "        -0.044042  , -0.03629166, -0.02139004,  0.06225822,  0.06749981,\n",
       "         0.04025536, -0.05094245, -0.07716875,  0.01367581,  0.04804884,\n",
       "        -0.06411732,  0.08012497,  0.06488223, -0.0401676 , -0.0043748 ,\n",
       "        -0.02983341,  0.03791076, -0.03484757,  0.01302149, -0.01841161,\n",
       "        -0.01783686,  0.06025846, -0.05678686, -0.07017205,  0.0075669 ,\n",
       "        -0.00172496, -0.00851688, -0.0551251 ,  0.01644095, -0.07157466,\n",
       "         0.04663438, -0.04544251, -0.0043297 ,  0.0188331 ,  0.0376044 ,\n",
       "         0.02582847,  0.03558962,  0.06422112, -0.04272004,  0.01893624,\n",
       "         0.0358541 , -0.02856325,  0.00074525, -0.0736227 ,  0.00259439,\n",
       "        -0.04074443,  0.03501793,  0.00051326,  0.00654907,  0.02997935,\n",
       "        -0.05977792, -0.07766525,  0.01968518, -0.0468813 , -0.07022466,\n",
       "         0.02891445, -0.0614684 , -0.01335105, -0.0156142 ,  0.04840892,\n",
       "        -0.03265833, -0.00048041, -0.04615324,  0.05808984,  0.03711191,\n",
       "        -0.0366994 ,  0.06909052,  0.03732014,  0.07337499, -0.06110212,\n",
       "        -0.03898143, -0.06020235, -0.01202926,  0.05333276,  0.0101274 ,\n",
       "        -0.02368968, -0.03910351, -0.05409376,  0.01412339, -0.05268003,\n",
       "         0.06032589, -0.05867278,  0.02881704,  0.05494995,  0.02341572,\n",
       "         0.01757453,  0.03235894, -0.06686524,  0.07158142,  0.04337487,\n",
       "        -0.04361833, -0.06452913,  0.05813808, -0.01484615,  0.003262  ,\n",
       "         0.00532469, -0.02899806, -0.03829428, -0.01069209,  0.05709114,\n",
       "         0.06743345,  0.02489435,  0.06228745, -0.05099756,  0.00306051,\n",
       "         0.01570034,  0.04915618,  0.01261293, -0.00017388,  0.07173525,\n",
       "        -0.03428971,  0.05018793, -0.01658629, -0.02687996,  0.05589588,\n",
       "        -0.02759081,  0.00872208, -0.0198071 , -0.07236175,  0.0197673 ,\n",
       "         0.04588206, -0.01921182,  0.04514967, -0.06470078,  0.01487608,\n",
       "        -0.02304718, -0.02925863,  0.0154969 , -0.05510686, -0.07941134,\n",
       "         0.02346326,  0.02760775,  0.01985027,  0.01358672,  0.00078478,\n",
       "         0.06426813,  0.04938675,  0.01520447, -0.05341157,  0.03346766,\n",
       "         0.02364513, -0.02940764, -0.04230523, -0.02258528, -0.06102028,\n",
       "         0.04183885, -0.0451648 , -0.01962263,  0.06051249, -0.05936494,\n",
       "        -0.0299089 , -0.06360215,  0.06147806, -0.05160561,  0.04800707,\n",
       "        -0.03772851, -0.02118205, -0.05578393,  0.03220291, -0.04059085,\n",
       "         0.04688715, -0.02527152, -0.03222938, -0.00925207, -0.05637441,\n",
       "        -0.05437814,  0.01944971, -0.01319102, -0.02462999,  0.01604836,\n",
       "         0.05972254, -0.03581974,  0.0295679 ,  0.00060033, -0.01534042,\n",
       "         0.00469192, -0.0435823 , -0.04812265,  0.0650899 ,  0.051337  ,\n",
       "        -0.02103614,  0.02665536,  0.01986725, -0.06211832, -0.00225836,\n",
       "         0.02575984, -0.01147407,  0.06487608, -0.01157435, -0.01857979,\n",
       "        -0.0748225 , -0.04577564,  0.00405795,  0.0094645 , -0.06200308,\n",
       "        -0.02572168, -0.05589885, -0.00201956,  0.04083333, -0.07584581,\n",
       "        -0.07231305, -0.06025453, -0.03670178,  0.03360886, -0.05783967,\n",
       "         0.06887657,  0.05280484,  0.03620063,  0.03344937,  0.06638525,\n",
       "        -0.0444939 ,  0.0494012 ,  0.01943135,  0.02463733,  0.07646743,\n",
       "         0.03408346, -0.04556011, -0.06916714,  0.03161265, -0.06478589,\n",
       "        -0.06415462, -0.00819282,  0.07940535,  0.01301914,  0.00686228,\n",
       "        -0.04612719, -0.02141312, -0.0408279 , -0.0691866 , -0.00861357,\n",
       "        -0.03169538,  0.04701381,  0.05474211, -0.07178418,  0.06532722,\n",
       "        -0.05893271, -0.03253707,  0.04933558,  0.02549374, -0.03783261,\n",
       "         0.0493351 ,  0.01033118, -0.0764825 , -0.05869344,  0.03737289,\n",
       "        -0.02633121,  0.02353063, -0.01722927, -0.06051085,  0.05357246,\n",
       "        -0.05490647,  0.01119267,  0.01664922, -0.06919731, -0.00868941,\n",
       "         0.06649305, -0.03623306, -0.06060735,  0.04468112, -0.04138396,\n",
       "        -0.0227385 , -0.06563935,  0.00439033, -0.06462386, -0.01630274,\n",
       "        -0.06495462, -0.06004854,  0.06611467, -0.00038597, -0.03528113,\n",
       "        -0.01856   ,  0.07085437, -0.04198383, -0.05147941,  0.02338981,\n",
       "        -0.04034049,  0.06681217,  0.03432931,  0.05056621, -0.01829554,\n",
       "         0.05006558,  0.007528  , -0.02706488,  0.02628221,  0.05133124,\n",
       "        -0.07227507, -0.03257197,  0.01504453,  0.05911594,  0.02928635,\n",
       "         0.05478209,  0.0321743 ,  0.00281225, -0.02909453,  0.03100142,\n",
       "        -0.05955438,  0.07665598,  0.05949254, -0.01138042,  0.05008577,\n",
       "        -0.00185451, -0.01315076,  0.03441036, -0.00033628, -0.05758049,\n",
       "        -0.01821719, -0.04637909, -0.0491057 , -0.04715162, -0.06098917,\n",
       "        -0.04704331, -0.05331867, -0.01408055, -0.03322227,  0.02112328,\n",
       "         0.01031214, -0.05431982, -0.02948329,  0.00245004,  0.07475778,\n",
       "        -0.0732437 ,  0.03426068, -0.00198169,  0.00255848, -0.02632344,\n",
       "        -0.03203675,  0.04333641,  0.00753297,  0.00981167, -0.02755754,\n",
       "         0.05738176, -0.07763284,  0.06129367, -0.05846645, -0.06576087,\n",
       "         0.0435444 , -0.04659993,  0.05236191,  0.03218196, -0.04393181,\n",
       "        -0.03236551,  0.0618302 , -0.07080087, -0.03328775, -0.0376709 ,\n",
       "         0.02548879,  0.03242898,  0.00789046, -0.03336541, -0.07493926,\n",
       "         0.05218091,  0.07345758, -0.06790037,  0.01362463, -0.06207278,\n",
       "        -0.00420277, -0.06427439, -0.02260612, -0.04098238, -0.04345876,\n",
       "        -0.03571404, -0.0309331 ,  0.02502772, -0.00421964,  0.04412623,\n",
       "        -0.01852098, -0.01697886, -0.05345034, -0.0365101 , -0.07470217,\n",
       "        -0.07254052,  0.0722359 , -0.00626548,  0.02457965,  0.04100616,\n",
       "         0.05663348,  0.01229839, -0.05439502, -0.04743414, -0.00072423,\n",
       "         0.0300916 , -0.03347421, -0.05896327, -0.06931324, -0.01728322,\n",
       "        -0.08012937,  0.00172605, -0.02704496,  0.06966414, -0.06070003,\n",
       "        -0.02979277, -0.0138573 ]], dtype=float32)>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_sample = embed([random_sentence])\n",
    "embed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoder = hub.KerasLayer(embed,\n",
    "                                  input_shape=[],\n",
    "                                  dtype=tf.string,\n",
    "                                  trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        \n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 5)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[117], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m model_2 \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[1;32m      2\u001b[0m     sentence_encoder,\n\u001b[1;32m      3\u001b[0m     tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m1\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m ])\n\u001b[1;32m      6\u001b[0m model_2\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m      7\u001b[0m               optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(),\n\u001b[1;32m      8\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m history_2 \u001b[38;5;241m=\u001b[39m model_2\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     10\u001b[0m     train_dataset,\n\u001b[1;32m     11\u001b[0m     epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     12\u001b[0m     steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset),\n\u001b[1;32m     13\u001b[0m     validation_data \u001b[38;5;241m=\u001b[39m val_dataset,\n\u001b[1;32m     14\u001b[0m     validation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_dataset))\n\u001b[1;32m     15\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/4p/86fqrsqj6z36dr_23z25gf8m0000gn/T/__autograph_generated_file7sutqyl0.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1151, in train_step\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 143, in __call__\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 270, in call  **\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/losses.py\", line 2532, in binary_crossentropy\n        \n    File \"/Users/suchirmvelpanur/anaconda3/lib/python3.11/site-packages/keras/src/backend.py\", line 5822, in binary_crossentropy\n        \n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 1) vs (None, 5)).\n"
     ]
    }
   ],
   "source": [
    "model_2 = tf.keras.models.Sequential([\n",
    "    sentence_encoder,\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_2.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "history_2 = model_2.fit(\n",
    "    train_dataset,\n",
    "    epochs = 5,\n",
    "    steps_per_epoch = len(train_dataset),\n",
    "    validation_data = val_dataset,\n",
    "    validation_steps = int(0.2 * len(val_dataset))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
