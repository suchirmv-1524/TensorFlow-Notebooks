{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Notebook\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile \n",
    "zip = zipfile.ZipFile(\"nlp_getting_started.zip\")\n",
    "zip.extractall()\n",
    "zip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"nlp_getting_started/train.csv\")\n",
    "test_df = pd.read_csv(\"nlp_getting_started/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 0; not a real disaster\n",
      "Text:  Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\n",
      "----------------------------------------\n",
      "Target: 1; real disaster\n",
      "Text:  S61.231A Puncture wound without foreign body of left index finger without damage to nail initial encounter #icd10\n",
      "----------------------------------------\n",
      "Target: 1; real disaster\n",
      "Text:  Udhampur terror attack: Militants attack police post 2 SPOs injured http://t.co/zMWeCBWVaO\n",
      "----------------------------------------\n",
      "Target: 0; not a real disaster\n",
      "Text:  The once desolate valley was transformed into a thriving hub of hiÛÓtech business.\n",
      "----------------------------------------\n",
      "Target: 0; not a real disaster\n",
      "Text:  if firefighters acted like cops they'd drive around shooting a flamethrower at burning buildings\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_index = random.randint(0, len(train_df) - 1)\n",
    "for row in train_df_shuffled[['text', 'target']][random_index : random_index + 5].itertuples():\n",
    "    index, text, target = row\n",
    "    print(f\"Target: {target}; {'real disaster' if target != 0 else 'not a real disaster'}\")\n",
    "    print(\"Text: \", text, end = \"\\n\")\n",
    "    print(\"--\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_df_shuffled['text'].to_numpy(),\n",
    "                                                                            train_df_shuffled['target'].to_numpy(),\n",
    "                                                                            test_size = 0.2,\n",
    "                                                                            random_state = 42)\n",
    "\n",
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6090, 1523)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sentences), len(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_length = 10000 #setting limit on number of words model can learn\n",
    "max_length = round(sum([len(i.split()) for i in train_sentences])/len(train_sentences))\n",
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text to numbers - text vectorisation\n",
    "text_vectorizer = tf.keras.layers.TextVectorization(max_tokens = max_vocab_length,\n",
    "                                                 output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A young heavyweight rapping off of detonate I been a leader not a lemon better get it straight ?? \n",
      " 97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[   3, 1003,    1, 4387,  102,    6,  507,    8,   61,    3, 1908,\n",
       "          34,    3, 4793,  453]])>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_sentence = random.choice(train_sentences)\n",
    "print(random_sentence, '\\n', len(random_sentence))\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " 'the',\n",
       " 'a',\n",
       " 'in',\n",
       " 'to',\n",
       " 'of',\n",
       " 'and',\n",
       " 'i',\n",
       " 'is',\n",
       " 'for',\n",
       " 'on',\n",
       " 'you',\n",
       " 'my',\n",
       " 'with',\n",
       " 'it',\n",
       " 'that',\n",
       " 'at',\n",
       " 'by',\n",
       " 'this',\n",
       " 'from',\n",
       " 'be',\n",
       " 'are',\n",
       " 'was',\n",
       " 'have',\n",
       " 'like',\n",
       " 'as',\n",
       " 'so',\n",
       " 'up',\n",
       " 'im',\n",
       " 'but',\n",
       " 'just',\n",
       " 'me',\n",
       " 'your',\n",
       " 'not',\n",
       " 'amp',\n",
       " 'out',\n",
       " 'its',\n",
       " 'no',\n",
       " 'has',\n",
       " 'will',\n",
       " 'all',\n",
       " 'an',\n",
       " 'after',\n",
       " 'fire',\n",
       " 'when',\n",
       " 'if',\n",
       " 'via',\n",
       " 'we',\n",
       " 'now',\n",
       " 'get',\n",
       " 'new',\n",
       " 'more',\n",
       " 'or',\n",
       " 'what',\n",
       " 'people',\n",
       " 'over',\n",
       " 'news',\n",
       " 'about',\n",
       " 'he',\n",
       " 'dont',\n",
       " 'been',\n",
       " 'how',\n",
       " 'who',\n",
       " 'they',\n",
       " 'one',\n",
       " 'into',\n",
       " 'do',\n",
       " 'were',\n",
       " 'us',\n",
       " 'video',\n",
       " '2',\n",
       " 'emergency',\n",
       " 'disaster',\n",
       " 'can',\n",
       " 'there',\n",
       " 'his',\n",
       " 'than',\n",
       " 'her',\n",
       " 'still',\n",
       " 'would',\n",
       " 'storm',\n",
       " 'some',\n",
       " 'police',\n",
       " 'body',\n",
       " 'them',\n",
       " 'crash',\n",
       " 'back',\n",
       " 'suicide',\n",
       " 'man',\n",
       " 'burning',\n",
       " 'why',\n",
       " 'time',\n",
       " 'day',\n",
       " 'california',\n",
       " 'rt',\n",
       " 'first',\n",
       " 'see',\n",
       " 'had',\n",
       " 'going',\n",
       " 'world',\n",
       " 'nuclear',\n",
       " 'off',\n",
       " 'know',\n",
       " 'buildings',\n",
       " 'bomb',\n",
       " 'got',\n",
       " 'cant',\n",
       " 'our',\n",
       " 'youtube',\n",
       " 'love',\n",
       " 'car',\n",
       " 'attack',\n",
       " 'killed',\n",
       " 'fires',\n",
       " 'train',\n",
       " 'go',\n",
       " 'families',\n",
       " 'war',\n",
       " 'could',\n",
       " 'being',\n",
       " 'two',\n",
       " 'many',\n",
       " 'life',\n",
       " 'accident',\n",
       " '3',\n",
       " 'their',\n",
       " 'dead',\n",
       " 'watch',\n",
       " 'want',\n",
       " 'full',\n",
       " 'say',\n",
       " 'only',\n",
       " 'may',\n",
       " 'think',\n",
       " 'hiroshima',\n",
       " 'good',\n",
       " 'today',\n",
       " 'here',\n",
       " 'did',\n",
       " 'make',\n",
       " 'last',\n",
       " 'way',\n",
       " 'years',\n",
       " 'u',\n",
       " 'then',\n",
       " 'down',\n",
       " 'best',\n",
       " 'am',\n",
       " 'help',\n",
       " 'work',\n",
       " 'wildfire',\n",
       " 'take',\n",
       " 'mass',\n",
       " 'home',\n",
       " 'even',\n",
       " 'collapse',\n",
       " 'too',\n",
       " 'look',\n",
       " 'lol',\n",
       " 'death',\n",
       " 'black',\n",
       " 'really',\n",
       " 'please',\n",
       " 'mh370',\n",
       " 'fatal',\n",
       " 'legionnaires',\n",
       " 'another',\n",
       " 'wreck',\n",
       " 'pm',\n",
       " 'need',\n",
       " 'army',\n",
       " 'youre',\n",
       " 'year',\n",
       " 'bombing',\n",
       " 'much',\n",
       " 'let',\n",
       " 'god',\n",
       " 'city',\n",
       " 'water',\n",
       " 'those',\n",
       " 'should',\n",
       " 'northern',\n",
       " 'because',\n",
       " '4',\n",
       " 'right',\n",
       " 'live',\n",
       " 'hot',\n",
       " 'him',\n",
       " 'great',\n",
       " 'forest',\n",
       " '1',\n",
       " '5',\n",
       " 'under',\n",
       " 'top',\n",
       " 'obama',\n",
       " 'homes',\n",
       " 'bomber',\n",
       " 'never',\n",
       " 'latest',\n",
       " 'getting',\n",
       " 'every',\n",
       " 'any',\n",
       " '2015',\n",
       " '\\x89Û',\n",
       " 'school',\n",
       " 'come',\n",
       " 'since',\n",
       " 'she',\n",
       " 'said',\n",
       " 'old',\n",
       " 'floods',\n",
       " 'thats',\n",
       " 'read',\n",
       " 'japan',\n",
       " 'found',\n",
       " 'flooding',\n",
       " 'feel',\n",
       " 'fear',\n",
       " 'ever',\n",
       " 'evacuation',\n",
       " 'before',\n",
       " 'without',\n",
       " 'while',\n",
       " 'these',\n",
       " 'damage',\n",
       " 'content',\n",
       " 'shit',\n",
       " 'near',\n",
       " 'flood',\n",
       " 'atomic',\n",
       " 'stop',\n",
       " 'malaysia',\n",
       " 'hit',\n",
       " 'fucking',\n",
       " 'debris',\n",
       " 'bloody',\n",
       " 'which',\n",
       " 'weather',\n",
       " 'weapon',\n",
       " 'theres',\n",
       " 'most',\n",
       " 'food',\n",
       " 'flames',\n",
       " 'evacuate',\n",
       " 'during',\n",
       " 'ass',\n",
       " 'wild',\n",
       " 'sinking',\n",
       " 'severe',\n",
       " 's',\n",
       " 'night',\n",
       " 'military',\n",
       " 'confirmed',\n",
       " 'again',\n",
       " 'where',\n",
       " 'truck',\n",
       " 'times',\n",
       " 'reddit',\n",
       " 'plan',\n",
       " 'oil',\n",
       " 'little',\n",
       " 'injuries',\n",
       " 'free',\n",
       " 'fall',\n",
       " 'derailment',\n",
       " 'cross',\n",
       " 'coming',\n",
       " 'bad',\n",
       " 'always',\n",
       " 'thunderstorm',\n",
       " 'natural',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'injured',\n",
       " 'face',\n",
       " 'explosion',\n",
       " 'well',\n",
       " 'through',\n",
       " 'refugees',\n",
       " 'panic',\n",
       " 'outbreak',\n",
       " 'murder',\n",
       " 'ive',\n",
       " 'hope',\n",
       " 'everyone',\n",
       " 'does',\n",
       " 'check',\n",
       " 'thunder',\n",
       " 'state',\n",
       " 'smoke',\n",
       " 'says',\n",
       " 'report',\n",
       " 'photo',\n",
       " 'next',\n",
       " 'hes',\n",
       " 'family',\n",
       " 'failure',\n",
       " 'explode',\n",
       " 'boy',\n",
       " 'blood',\n",
       " 'big',\n",
       " 'attacked',\n",
       " 'also',\n",
       " 'air',\n",
       " '70',\n",
       " '\\x89ÛÒ',\n",
       " 'wrecked',\n",
       " 'wounded',\n",
       " 'whole',\n",
       " 'weapons',\n",
       " 'twister',\n",
       " 'road',\n",
       " 'loud',\n",
       " 'liked',\n",
       " 'lightning',\n",
       " 'heat',\n",
       " 'fatalities',\n",
       " 'end',\n",
       " 'earthquake',\n",
       " 'destroy',\n",
       " 'charged',\n",
       " 'cause',\n",
       " 'catastrophe',\n",
       " 'wreckage',\n",
       " 'warning',\n",
       " 'w',\n",
       " 'violent',\n",
       " 'until',\n",
       " 'terrorism',\n",
       " 'survived',\n",
       " 'survive',\n",
       " 'summer',\n",
       " 'spill',\n",
       " 'set',\n",
       " 'services',\n",
       " 'saudi',\n",
       " 'rescuers',\n",
       " 'released',\n",
       " 'hurricane',\n",
       " 'house',\n",
       " 'head',\n",
       " 'dust',\n",
       " 'august',\n",
       " 'ambulance',\n",
       " 'windstorm',\n",
       " 'tragedy',\n",
       " 'tonight',\n",
       " 'terrorist',\n",
       " 'red',\n",
       " 'rain',\n",
       " 'migrants',\n",
       " 'looks',\n",
       " 'landslide',\n",
       " 'kills',\n",
       " 'injury',\n",
       " 'harm',\n",
       " 'hail',\n",
       " 'gonna',\n",
       " 'drought',\n",
       " 'destroyed',\n",
       " 'curfew',\n",
       " 'collided',\n",
       " 'breaking',\n",
       " 'boat',\n",
       " 'bag',\n",
       " '40',\n",
       " 'wind',\n",
       " 'survivors',\n",
       " 'structural',\n",
       " 'stock',\n",
       " 'service',\n",
       " 'saw',\n",
       " 'save',\n",
       " 'sandstorm',\n",
       " 'rescue',\n",
       " 'oh',\n",
       " 'missing',\n",
       " 'massacre',\n",
       " 'lives',\n",
       " 'horrible',\n",
       " 'high',\n",
       " 'girl',\n",
       " 'deaths',\n",
       " 'bags',\n",
       " 'women',\n",
       " 'white',\n",
       " 'trauma',\n",
       " 'story',\n",
       " 'sinkhole',\n",
       " 'screaming',\n",
       " 'riot',\n",
       " 'real',\n",
       " 'quarantined',\n",
       " 'quarantine',\n",
       " 'mosque',\n",
       " 'market',\n",
       " 'keep',\n",
       " 'island',\n",
       " 'investigators',\n",
       " 'ill',\n",
       " 'game',\n",
       " 'displaced',\n",
       " 'derail',\n",
       " 'crashed',\n",
       " 'cliff',\n",
       " 'call',\n",
       " 'apocalypse',\n",
       " 'affected',\n",
       " 'update',\n",
       " 'trapped',\n",
       " 'put',\n",
       " 'power',\n",
       " 'part',\n",
       " 'must',\n",
       " 'lava',\n",
       " 'engulfed',\n",
       " 'destruction',\n",
       " 'danger',\n",
       " 'collapsed',\n",
       " 'change',\n",
       " 'came',\n",
       " 'bus',\n",
       " 'blown',\n",
       " 'battle',\n",
       " 'away',\n",
       " 'area',\n",
       " 'airplane',\n",
       " '15',\n",
       " 'woman',\n",
       " 'wanna',\n",
       " 'used',\n",
       " 'typhoon',\n",
       " 'trouble',\n",
       " 'someone',\n",
       " 'send',\n",
       " 'run',\n",
       " 'ruin',\n",
       " 'reunion',\n",
       " 'rescued',\n",
       " 'post',\n",
       " 'phone',\n",
       " 'national',\n",
       " 'mudslide',\n",
       " 'long',\n",
       " 'inundated',\n",
       " 'heart',\n",
       " 'hazard',\n",
       " 'fuck',\n",
       " 'famine',\n",
       " 'drowning',\n",
       " 'devastation',\n",
       " 'deluge',\n",
       " 'better',\n",
       " 'armageddon',\n",
       " 'against',\n",
       " 'zone',\n",
       " 'use',\n",
       " 'twitter',\n",
       " 'traumatised',\n",
       " 'tornado',\n",
       " 'thank',\n",
       " 'sure',\n",
       " 'sunk',\n",
       " 'show',\n",
       " 'screamed',\n",
       " 'rioting',\n",
       " 'panicking',\n",
       " 'obliterated',\n",
       " 'minute',\n",
       " 'meltdown',\n",
       " 'iran',\n",
       " 'id',\n",
       " 'hundreds',\n",
       " 'hijacking',\n",
       " 'hazardous',\n",
       " 'goes',\n",
       " 'flattened',\n",
       " 'exploded',\n",
       " 'derailed',\n",
       " 'care',\n",
       " 'bridge',\n",
       " 'bagging',\n",
       " 'around',\n",
       " 'annihilated',\n",
       " 'airport',\n",
       " '9',\n",
       " 'went',\n",
       " 'volcano',\n",
       " 'thought',\n",
       " 'sue',\n",
       " 'responders',\n",
       " 'razed',\n",
       " 'officials',\n",
       " 'obliteration',\n",
       " 'obliterate',\n",
       " 'music',\n",
       " 'lets',\n",
       " 'land',\n",
       " 'kill',\n",
       " 'hostages',\n",
       " 'hostage',\n",
       " 'guys',\n",
       " 'group',\n",
       " 'electrocuted',\n",
       " 'drown',\n",
       " 'died',\n",
       " 'detonate',\n",
       " 'desolation',\n",
       " 'crush',\n",
       " 'cool',\n",
       " 'collide',\n",
       " 'caused',\n",
       " 'bombed',\n",
       " 'bang',\n",
       " 'week',\n",
       " 'upheaval',\n",
       " 'tsunami',\n",
       " 'st',\n",
       " 'south',\n",
       " 'sound',\n",
       " 'security',\n",
       " 'river',\n",
       " 'policy',\n",
       " 'pkk',\n",
       " 'past',\n",
       " 'other',\n",
       " 'ok',\n",
       " 'mp',\n",
       " 'making',\n",
       " 'lot',\n",
       " 'least',\n",
       " 'isis',\n",
       " 'government',\n",
       " 'evacuated',\n",
       " 'due',\n",
       " 'drowned',\n",
       " 'doing',\n",
       " 'didnt',\n",
       " 'crushed',\n",
       " 'county',\n",
       " 'collision',\n",
       " 'chemical',\n",
       " 'calgary',\n",
       " 'burned',\n",
       " 'blew',\n",
       " 'beautiful',\n",
       " 'anniversary',\n",
       " 'yet',\n",
       " 'whirlwind',\n",
       " 'very',\n",
       " 'trying',\n",
       " 'three',\n",
       " 'things',\n",
       " 'suspect',\n",
       " 'soon',\n",
       " 'something',\n",
       " 'site',\n",
       " 'sirens',\n",
       " 'screams',\n",
       " 'rainstorm',\n",
       " 'possible',\n",
       " 'north',\n",
       " 'light',\n",
       " 'islam',\n",
       " 'india',\n",
       " 'hell',\n",
       " 'heard',\n",
       " 'fun',\n",
       " 'fedex',\n",
       " 'electrocute',\n",
       " 'ebay',\n",
       " 'detonation',\n",
       " 'cyclone',\n",
       " 'catastrophic',\n",
       " 'casualties',\n",
       " 'both',\n",
       " 'blazing',\n",
       " 'bc',\n",
       " 'baby',\n",
       " 'almost',\n",
       " '8',\n",
       " '6',\n",
       " 'wave',\n",
       " 'ur',\n",
       " 'traffic',\n",
       " 'song',\n",
       " 'snowstorm',\n",
       " 'side',\n",
       " 'second',\n",
       " 'prebreak',\n",
       " 'place',\n",
       " 'pandemonium',\n",
       " 'nowplaying',\n",
       " 'murderer',\n",
       " 'men',\n",
       " 'media',\n",
       " 'longer',\n",
       " 'left',\n",
       " 'kids',\n",
       " 'israeli',\n",
       " 'horror',\n",
       " 'hellfire',\n",
       " 'hear',\n",
       " 'half',\n",
       " 'few',\n",
       " 'fatality',\n",
       " 'done',\n",
       " 'detonated',\n",
       " 'demolished',\n",
       " 'building',\n",
       " 'bleeding',\n",
       " 'believe',\n",
       " '\\x89ÛÓ',\n",
       " 'yes',\n",
       " 'wounds',\n",
       " 'words',\n",
       " 'thing',\n",
       " 'team',\n",
       " 'swallowed',\n",
       " 'shoulder',\n",
       " 'shot',\n",
       " 'remember',\n",
       " 'pic',\n",
       " 'person',\n",
       " 'nothing',\n",
       " 'lost',\n",
       " 'line',\n",
       " 'la',\n",
       " 'inside',\n",
       " 'hijack',\n",
       " 'health',\n",
       " 'far',\n",
       " 'doesnt',\n",
       " 'demolish',\n",
       " 'deluged',\n",
       " 'declares',\n",
       " 'days',\n",
       " 'crews',\n",
       " 'business',\n",
       " 'blast',\n",
       " 'bioterror',\n",
       " 'aircraft',\n",
       " '16yr',\n",
       " 'west',\n",
       " 'turkey',\n",
       " 'thanks',\n",
       " 'stay',\n",
       " 'soudelor',\n",
       " 'seismic',\n",
       " 'rubble',\n",
       " 'plane',\n",
       " 'order',\n",
       " 'name',\n",
       " 'memories',\n",
       " 'maybe',\n",
       " 'makes',\n",
       " 'job',\n",
       " 'issues',\n",
       " 'history',\n",
       " 'hijacker',\n",
       " 'hey',\n",
       " 'helicopter',\n",
       " 'fight',\n",
       " 'feeling',\n",
       " 'fan',\n",
       " 'eyes',\n",
       " 'deal',\n",
       " 'data',\n",
       " 'conclusively',\n",
       " 'children',\n",
       " 'casualty',\n",
       " 'bush',\n",
       " 'book',\n",
       " 'avalanche',\n",
       " 'anything',\n",
       " 'actually',\n",
       " '50',\n",
       " 'yeah',\n",
       " 'wont',\n",
       " 'whats',\n",
       " 'wait',\n",
       " 'typhoondevastated',\n",
       " 'trains',\n",
       " 'texas',\n",
       " 'such',\n",
       " 'start',\n",
       " 'siren',\n",
       " 'saved',\n",
       " 'saipan',\n",
       " 're\\x89Û',\n",
       " 'reuters',\n",
       " 'rd',\n",
       " 'probably',\n",
       " 'play',\n",
       " 'plans',\n",
       " 'own',\n",
       " 'outside',\n",
       " 'others',\n",
       " 'nearby',\n",
       " 'myself',\n",
       " 'low',\n",
       " 'literally',\n",
       " 'huge',\n",
       " 'having',\n",
       " 'flash',\n",
       " 'find',\n",
       " 'die',\n",
       " 'demolition',\n",
       " 'crisis',\n",
       " 'control',\n",
       " 'brown',\n",
       " 'bioterrorism',\n",
       " 'arson',\n",
       " 'amid',\n",
       " 'already',\n",
       " 'abc',\n",
       " '7',\n",
       " '20',\n",
       " '12',\n",
       " 'youth',\n",
       " 'yourself',\n",
       " 'waves',\n",
       " 'support',\n",
       " 'street',\n",
       " 'spot',\n",
       " 'space',\n",
       " 'signs',\n",
       " 'shooting',\n",
       " 'searching',\n",
       " 'same',\n",
       " 'picking',\n",
       " 'peace',\n",
       " 'omg',\n",
       " 'n',\n",
       " 'money',\n",
       " 'manslaughter',\n",
       " 'leather',\n",
       " 'hours',\n",
       " 'gt',\n",
       " 'gets',\n",
       " 'eyewitness',\n",
       " 'effect',\n",
       " 'desolate',\n",
       " 'country',\n",
       " 'center',\n",
       " 'bodies',\n",
       " 'blight',\n",
       " 'blaze',\n",
       " 'bigger',\n",
       " 'become',\n",
       " 'bar',\n",
       " 'american',\n",
       " '11yearold',\n",
       " 'wrong',\n",
       " 'womens',\n",
       " 'watching',\n",
       " 'trench',\n",
       " 'though',\n",
       " 'theyre',\n",
       " 'tell',\n",
       " 'stretcher',\n",
       " 'russian',\n",
       " 'reason',\n",
       " 'reactor',\n",
       " 'projected',\n",
       " 'photos',\n",
       " 'okay',\n",
       " 'move',\n",
       " 'meek',\n",
       " 'mayhem',\n",
       " 'major',\n",
       " 'hat',\n",
       " 'friends',\n",
       " 'flag',\n",
       " 'experts',\n",
       " 'everything',\n",
       " 'disea',\n",
       " 'damn',\n",
       " 'course',\n",
       " 'couple',\n",
       " 'child',\n",
       " 'called',\n",
       " 'ball',\n",
       " 'america',\n",
       " 'across',\n",
       " '30',\n",
       " '25',\n",
       " 'yours',\n",
       " 'win',\n",
       " 'village',\n",
       " 'usa',\n",
       " 'tv',\n",
       " 'totally',\n",
       " 'temple',\n",
       " 'talk',\n",
       " 'taken',\n",
       " 'ship',\n",
       " 'self',\n",
       " 'running',\n",
       " 'rise',\n",
       " 'refugio',\n",
       " 'pretty',\n",
       " 'officer',\n",
       " 'offensive',\n",
       " 'needs',\n",
       " 'nearly',\n",
       " 'm',\n",
       " 'lord',\n",
       " 'level',\n",
       " 'leave',\n",
       " 'ladies',\n",
       " 'lab',\n",
       " 'instead',\n",
       " 'hollywood',\n",
       " 'hate',\n",
       " 'hard',\n",
       " 'hailstorm',\n",
       " 'guy',\n",
       " 'giant',\n",
       " 'finally',\n",
       " 'crazy',\n",
       " 'costlier',\n",
       " 'coaches',\n",
       " 'chance',\n",
       " 'caught',\n",
       " 'case',\n",
       " 'banned',\n",
       " 'alone',\n",
       " 'ago',\n",
       " 'ablaze',\n",
       " 'united',\n",
       " 'uk',\n",
       " 'trust',\n",
       " 'tomorrow',\n",
       " 'star',\n",
       " 'sounds',\n",
       " 'reddits',\n",
       " 'radio',\n",
       " 'quiz',\n",
       " 'property',\n",
       " 'poor',\n",
       " 'online',\n",
       " 'official',\n",
       " 'mount',\n",
       " 'miners',\n",
       " 'might',\n",
       " 'mad',\n",
       " 'looking',\n",
       " 'learn',\n",
       " 'large',\n",
       " 'knock',\n",
       " 'issued',\n",
       " 'isnt',\n",
       " 'insurance',\n",
       " 'ignition',\n",
       " 'happy',\n",
       " 'gun',\n",
       " 'global',\n",
       " 'fukushima',\n",
       " 'feared',\n",
       " 'entire',\n",
       " 'emmerdale',\n",
       " 'east',\n",
       " 'drive',\n",
       " 'daily',\n",
       " 'computers',\n",
       " 'closed',\n",
       " 'christian',\n",
       " 'bring',\n",
       " 'bestnaijamade',\n",
       " 'beach',\n",
       " 'aug',\n",
       " 'anyone',\n",
       " 'annihilation',\n",
       " 'angry',\n",
       " 'added',\n",
       " '05',\n",
       " 'wow',\n",
       " 'wake',\n",
       " 'virgin',\n",
       " 'vehicle',\n",
       " 'tree',\n",
       " 'transport',\n",
       " 'toddler',\n",
       " 'thousands',\n",
       " 'taking',\n",
       " 'takes',\n",
       " 'subreddits',\n",
       " 'stand',\n",
       " 'shift',\n",
       " 'shes',\n",
       " 'seen',\n",
       " 'seeing',\n",
       " 'scared',\n",
       " 'russia',\n",
       " 'rock',\n",
       " 'reports',\n",
       " 'ready',\n",
       " 'pradesh',\n",
       " 'pick',\n",
       " 'pain',\n",
       " 'outrage',\n",
       " 'once',\n",
       " 'niggas',\n",
       " 'moment',\n",
       " 'mom',\n",
       " 'marks',\n",
       " 'madhya',\n",
       " 'jobs',\n",
       " 'human',\n",
       " 'heavy',\n",
       " 'haha',\n",
       " 'green',\n",
       " 'gop',\n",
       " 'gems',\n",
       " 'gbbo',\n",
       " 'front',\n",
       " 'france',\n",
       " 'former',\n",
       " 'fast',\n",
       " 'eye',\n",
       " 'earth',\n",
       " 'driver',\n",
       " 'drake',\n",
       " 'devastated',\n",
       " 'declaration',\n",
       " 'd',\n",
       " 'comes',\n",
       " 'cnn',\n",
       " 'china',\n",
       " 'blue',\n",
       " 'blizzard',\n",
       " 'bbc',\n",
       " 'ancient',\n",
       " 'aint',\n",
       " 'action',\n",
       " '13',\n",
       " '10',\n",
       " 'worst',\n",
       " 'wonder',\n",
       " 'wasnt',\n",
       " 'view',\n",
       " 'turned',\n",
       " 'turn',\n",
       " 'try',\n",
       " 'truth',\n",
       " 'town',\n",
       " 'thinking',\n",
       " 't',\n",
       " 'super',\n",
       " 'soul',\n",
       " 'sorry',\n",
       " 'sky',\n",
       " 'sign',\n",
       " 'shows',\n",
       " 'sea',\n",
       " 'scene',\n",
       " 'rly',\n",
       " 'risk',\n",
       " 'r',\n",
       " 'public',\n",
       " 'problem',\n",
       " 'pray',\n",
       " 'playing',\n",
       " 'planned',\n",
       " 'pamela',\n",
       " 'pakistan',\n",
       " 'nws',\n",
       " 'nigerian',\n",
       " 'muslims',\n",
       " 'mop',\n",
       " 'mode',\n",
       " 'link',\n",
       " 'led',\n",
       " 'hand',\n",
       " 'govt',\n",
       " 'glad',\n",
       " 'galactic',\n",
       " 'funtenna',\n",
       " 'flight',\n",
       " 'firefighters',\n",
       " 'film',\n",
       " 'favorite',\n",
       " 'enough',\n",
       " 'else',\n",
       " 'dude',\n",
       " 'downtown',\n",
       " 'don\\x89Ûªt',\n",
       " 'disease',\n",
       " 'deep',\n",
       " 'cree',\n",
       " 'class',\n",
       " 'chinas',\n",
       " 'chile',\n",
       " 'cdt',\n",
       " 'camp',\n",
       " 'british',\n",
       " 'biggest',\n",
       " 'bed',\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = text_vectorizer.get_vocabulary() #retrieves all words used in dataset as a vocabulary\n",
    "words \n",
    "# '[UNK]' means UNKNOWN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim = max_vocab_length,\n",
    "                                      output_dim = 120,\n",
    "                                      input_length = max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 120), dtype=float32, numpy=\n",
       "array([[[-0.04244869, -0.03680097, -0.02805817, ...,  0.04411365,\n",
       "         -0.02866998,  0.01786036],\n",
       "        [ 0.01437977, -0.03324739, -0.00604177, ..., -0.01516079,\n",
       "         -0.04671365,  0.01019373],\n",
       "        [ 0.00918283, -0.04909357, -0.03701273, ...,  0.04926488,\n",
       "          0.01956513,  0.00249987],\n",
       "        ...,\n",
       "        [-0.04244869, -0.03680097, -0.02805817, ...,  0.04411365,\n",
       "         -0.02866998,  0.01786036],\n",
       "        [ 0.0381521 ,  0.01924082, -0.04228556, ..., -0.00395843,\n",
       "          0.02309265, -0.01705937],\n",
       "        [-0.01951817,  0.02419404, -0.01492586, ..., -0.01188663,\n",
       "         -0.00550442, -0.00846777]]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = embedding(text_vectorizer([random_sentence]))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(120,), dtype=float32, numpy=\n",
       "array([-0.04244869, -0.03680097, -0.02805817, -0.04663133,  0.02923149,\n",
       "        0.04409691, -0.01188319, -0.00391092,  0.02141175, -0.02238894,\n",
       "       -0.02937688, -0.0340545 ,  0.01052625, -0.04886233,  0.01378752,\n",
       "       -0.04016966,  0.04714585,  0.04233185,  0.00229738,  0.02793683,\n",
       "       -0.0152971 ,  0.02105315, -0.00784013, -0.02254069,  0.02612816,\n",
       "        0.03484404, -0.0366744 , -0.03547295, -0.03939867,  0.01134398,\n",
       "       -0.04432642, -0.03725815, -0.02055209,  0.00555314,  0.02360162,\n",
       "       -0.00315198,  0.04300695,  0.01287955, -0.00634867,  0.02777685,\n",
       "        0.02099793,  0.04763869,  0.04546868,  0.00012308, -0.03878796,\n",
       "       -0.04406376,  0.02074586,  0.03911132, -0.00931199, -0.02762206,\n",
       "       -0.00795136,  0.02071022, -0.04422308,  0.000994  , -0.01855528,\n",
       "       -0.01222867,  0.02672818, -0.03936536,  0.04385774,  0.0022599 ,\n",
       "        0.03657756,  0.0411781 ,  0.04997564,  0.04583656,  0.0093592 ,\n",
       "       -0.03437432, -0.03855877, -0.04036175, -0.01035436,  0.00132791,\n",
       "        0.02064729, -0.02055898, -0.02500676,  0.02818643,  0.049709  ,\n",
       "       -0.01239363, -0.01067257, -0.00972108,  0.02766291, -0.0367283 ,\n",
       "       -0.00479406,  0.0369035 ,  0.01016307,  0.0196136 , -0.04809532,\n",
       "        0.01170602,  0.02102311, -0.02140027, -0.04648867, -0.02351047,\n",
       "       -0.0363589 ,  0.02751602,  0.01166584, -0.02113833, -0.00690241,\n",
       "       -0.03239746,  0.03380876, -0.01479769,  0.01785889,  0.02781651,\n",
       "       -0.04326659, -0.01878463,  0.01988721, -0.00603886, -0.00401131,\n",
       "        0.01555175,  0.00208429,  0.03142134,  0.00969402,  0.04139552,\n",
       "        0.04129329,  0.02441064, -0.00052235,  0.00243365,  0.04113623,\n",
       "        0.03883961,  0.04280286,  0.04411365, -0.02866998,  0.01786036],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0][0] #for each word there's a vector assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.layers.Input(shape = (1, ), dtype = 'string') #shape = (1, ) because only 1 sentence is getting processed\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D()(x) #1D because we're only having input_dim = 1\n",
    "outputs = tf.keras.layers.Dense(1, activation = 'sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer = tf.keras.optimizers.legacy.Adam(),\n",
    "              metrics =  ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_15 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization (TextVe  (None, 15)                0         \n",
      " ctorization)                                                    \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 120)           1200000   \n",
      "                                                                 \n",
      " global_average_pooling1d_4  (None, 120)               0         \n",
      "  (GlobalAveragePooling1D)                                       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 121       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1200121 (4.58 MB)\n",
      "Trainable params: 1200121 (4.58 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  1/191 [..............................] - ETA: 0s - loss: 0.0145 - accuracy: 1.0000"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0376 - accuracy: 0.9819 - val_loss: 1.1891 - val_accuracy: 0.7617\n",
      "Epoch 2/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0376 - accuracy: 0.9816 - val_loss: 1.2158 - val_accuracy: 0.7597\n",
      "Epoch 3/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0366 - accuracy: 0.9826 - val_loss: 1.2341 - val_accuracy: 0.7551\n",
      "Epoch 4/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0368 - accuracy: 0.9814 - val_loss: 1.2462 - val_accuracy: 0.7603\n",
      "Epoch 5/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0365 - accuracy: 0.9824 - val_loss: 1.2652 - val_accuracy: 0.7577\n",
      "Epoch 6/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0361 - accuracy: 0.9811 - val_loss: 1.2846 - val_accuracy: 0.7577\n",
      "Epoch 7/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0356 - accuracy: 0.9831 - val_loss: 1.2926 - val_accuracy: 0.7557\n",
      "Epoch 8/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0358 - accuracy: 0.9819 - val_loss: 1.3174 - val_accuracy: 0.7525\n",
      "Epoch 9/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0348 - accuracy: 0.9841 - val_loss: 1.3327 - val_accuracy: 0.7551\n",
      "Epoch 10/10\n",
      "191/191 [==============================] - 1s 4ms/step - loss: 0.0355 - accuracy: 0.9823 - val_loss: 1.3357 - val_accuracy: 0.7531\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(tf.expand_dims(train_sentences, axis = 1), train_labels,\n",
    "          epochs = 10,\n",
    "          validation_data = (val_sentences, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
